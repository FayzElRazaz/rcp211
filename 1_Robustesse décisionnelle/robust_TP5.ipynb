{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "508bf98a",
   "metadata": {},
   "source": [
    "\n",
    "<a id='chap-tprs5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df1d414",
   "metadata": {},
   "source": [
    "# Practical session 5: Out-of-distribution detection\n",
    "\n",
    "You can open this practical session in colab here :\n",
    "[https://colab.research.google.com/drive/1CFM5sCZDkzK7BKBCH8-U2pbNORkfvBH1?usp=sharing](https://colab.research.google.com/drive/1CFM5sCZDkzK7BKBCH8-U2pbNORkfvBH1?usp=sharing)\n",
    "\n",
    "or download the notebook directly [here](http://cedric.cnam.fr/~rambourc/TP5_RCP211_failure_OOD.ipynb).\n",
    "\n",
    "This last lab session will focus on 2 examples where good uncertainty estimation is crucial :\n",
    "failure prediction and out-of-distribution detection.\n",
    "\n",
    "**Goal**: Take hand on applying uncertainty estimation for out-of-distribution detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67eae6fb",
   "metadata": {},
   "source": [
    "## All Imports and Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f38a793",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from IPython import display\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib.pyplot import imread\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "use_cuda = True\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "kwargs = {}\n",
    "#kwargs = {'num_workers': 10, 'pin_memory': True} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21786085",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def plot_predicted_images(selected_idx, images, pred, labels, uncertainties, hists, mc_samples):\n",
    "    \"\"\" Plot predicted images along with mean-pred probabilities histogram, maxprob frequencies and some class histograms\n",
    "    across sampliong\n",
    "\n",
    "    Args:\n",
    "    selected_ix: (array) chosen index in the uncertainties tensor\n",
    "    images: (tensor) images from the test set\n",
    "    pred: (tensor) class predictions by the model\n",
    "    labels: (tensor) true labels of the given dataset\n",
    "    uncertainties: (tensor) uncertainty estimates of the given dataset\n",
    "    errors: (tensor) 0/1 vector whether the model wrongly predicted a sample\n",
    "    hists : (array) number of occurences by class in each sample fo the given dataset, only with MCDropout\n",
    "    mc_samples: (tensor) prediction matrix for s=100 samples, only with MCDropout\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15,10))\n",
    "\n",
    "    for i, idx in enumerate(selected_idx):\n",
    "        # Plot original image\n",
    "        plt.subplot(5,6,1+6*i)\n",
    "        plt.axis('off')\n",
    "        plt.title(f'var-ratio={uncertainties[idx]:.3f}, \\n gt={labels[idx]}, pred={pred[idx]}')\n",
    "        plt.imshow(images[idx], cmap='gray')\n",
    "\n",
    "        # Plot mean probabilities\n",
    "        plt.subplot(5,6,1+6*i+1)\n",
    "        plt.title('Mean probs')\n",
    "        plt.bar(range(10), mc_samples[idx].mean(0))\n",
    "        plt.xticks(range(10))\n",
    "\n",
    "        # Plot frequencies\n",
    "        plt.subplot(5,6,1+6*i+2)\n",
    "        plt.title('Maxprob frequencies')\n",
    "        plt.bar(range(10), hists[idx])\n",
    "        plt.xticks(range(10))\n",
    "\n",
    "        # Plot probs frequency for specific class\n",
    "        list_plotprobs = [hists[idx].argsort()[-1], hists[idx].argsort()[-2], hists[idx].argsort()[-4]]\n",
    "        ymax = max([max(np.histogram(mc_samples[idx][:,c])[0]) for c in list_plotprobs])\n",
    "        for j, c in enumerate(list_plotprobs):\n",
    "            plt.subplot(5,6,1+6*i+(3+j))\n",
    "            plt.title(f'Samples probs of class {c}')\n",
    "            plt.hist(mc_samples[idx][:,c], bins=np.arange(0,1.1,0.1))\n",
    "            plt.ylim(0,np.ceil(ymax/10)*10)\n",
    "            plt.xticks(np.arange(0,1,0.1), rotation=60)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c753432",
   "metadata": {},
   "source": [
    "## Out-of-distribution detection\n",
    "\n",
    "Modern neural networks are known to generalize well when the training and testing data\n",
    "are sampled from the same distribution. However, when deploying neural networks in real-world\n",
    "applications, there is often very little control over the testing data distribution. It is\n",
    "important for classifiers to be aware of uncertainty when shown new kinds of inputs, i.e., out-of-distribution\n",
    "examples. Therefore, being able to accurately detect out-of-distribution examples can be practically important\n",
    "for visual recognition tasks.\n",
    "\n",
    "<img src=\"Bloc_Robust/images/grumpy.png\" alt=\"dogs and Cats\" style=\"width:600;\">\n",
    "\n",
    "In this section, we will use Kuzushiji-MNIST, a drop-in replacement for the MNIST\n",
    "dataset (28x28 grayscale, 70,000 images) containing 3832 Kanji (japanese) characters, as out-of-distribution\n",
    "sample to our model trained on MNIST. We will compare the methods for uncertainty estimates used previously and ODIN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e689933c",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Load KMNIST dataset\n",
    "kmnist_test_dataset = datasets.KMNIST('data', train=False, download=True, transform=transform)\n",
    "kmnist_test_loader = DataLoader(kmnist_test_dataset, batch_size=128)\n",
    "\n",
    "# Visualize some images\n",
    "images, labels = next(iter(kmnist_test_loader))\n",
    "fig, axes = plt.subplots(nrows=4, ncols=4)\n",
    "for i, (image, label) in enumerate(zip(images, labels)):\n",
    "    if i >= 16:\n",
    "        break\n",
    "    axes[i // 4][i % 4].imshow(images[i][0], cmap='gray')\n",
    "    axes[i // 4][i % 4].set_title(f\"{kmnist_test_dataset.classes[label]}\")\n",
    "    axes[i // 4][i % 4].set_xticks([])\n",
    "    axes[i // 4][i % 4].set_yticks([])\n",
    "fig.set_size_inches(4, 4)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba98bf4b",
   "metadata": {},
   "source": [
    "We compute the precision, recall and AUPR metric for OOD detection with MCP and MCDrpoout with mutual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2602c046",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Compute predictions for MCP method on MNIST\n",
    "_, _, uncertainty_mcp, errors_mcp, _, _ = predict_test_set(lenet, test_loader, mode='mcp')\n",
    "\n",
    "# Same on KMNIST\n",
    "_, _, uncertainty_kmnist, errors_kmnist, _, _ = predict_test_set(lenet, kmnist_test_loader, mode='mcp')\n",
    "\n",
    "# Concatenating predictions with MNIST, considering KMNIST samples as out-of-distributions\n",
    "tot_uncertainty = np.concatenate((uncertainty_mcp, uncertainty_kmnist))\n",
    "in_distribution = np.concatenate((np.zeros_like(uncertainty_mcp), np.ones_like(uncertainty_kmnist)))\n",
    "\n",
    "# Obtaining precision and recall plot vector + AUPR\n",
    "precision_ood_mcp, recall_ood_mcp, _ = precision_recall_curve(in_distribution, -tot_uncertainty)\n",
    "aupr_ood_mcp = average_precision_score(in_distribution, -tot_uncertainty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6809eb2d",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Computing for MCDropout with entropy\n",
    "_, _, uncertainty_mutinf, _, _, _ = predict_test_set(lenet, test_loader, mode='mut_inf')\n",
    "_, _, uncertainty_mutinf_kmnist, _, _, _ = predict_test_set(lenet, kmnist_test_loader, mode='mut_inf')\n",
    "tot_uncertainty = np.concatenate((uncertainty_mutinf, uncertainty_mutinf_kmnist))\n",
    "in_distribution = np.concatenate((np.zeros_like(uncertainty_mutinf), np.ones_like(uncertainty_mutinf_kmnist)))\n",
    "\n",
    "precision_ood_ent, recall_ood_ent, _ = precision_recall_curve(in_distribution, tot_uncertainty)\n",
    "aupr_ood_ent = average_precision_score(in_distribution, tot_uncertainty)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015ebe0e",
   "metadata": {},
   "source": [
    "We will now implement the ODIN method.\n",
    "\n",
    "ODIN [[Liang et al., ICLR 2018]](https://openreview.net/pdf?id=H1VGkIxRZ),\n",
    "is a threshold-based detector enhancing maximum softmax probabilities with two extensions:\n",
    "\n",
    "- **temperature scaling**: $ \\textit{p}(y= c \\vert \\mathbf{x}, \\mathbf{w}, T) = \\frac{\\exp(f_c( \\mathbf{x}, \\mathbf{w}) / T)}{\\sum_{k=1}^K \\exp(f_k( \\mathbf{x}, \\mathbf{w}) / T)} $  \n",
    "\n",
    "\n",
    "where $ T \\in \\mathbb{R}^{+} $\n",
    "\n",
    "- **inverse adversarial perturbation**: $ \\tilde{\\mathbf{x}} = \\mathbf{x} - \\epsilon \\mathrm{sign} \\big ( - \\nabla_x \\log (\\textit{p}(y = y^* \\vert \\mathbf{x}, \\mathbf{w}, T) \\big ) $  \n",
    "\n",
    "\n",
    "Both technics aimed to increase in-distribution MCP\n",
    "higher than out-distribution MCP. Here, we set the hyperparameters $ T=5 $ and $ \\epsilon=0.0014 $.\n",
    "\n",
    "**[CODING TASK]** Implement ODIN preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd033a52",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def odin_preprocessing(model, input, epsilon):\n",
    "    # We perform the invese adversarial perturbation\n",
    "    # You can find some help in the link below:\n",
    "    # https://pytorch.org/tutorials/beginner/fgsm_tutorial.html\n",
    "\n",
    "    # ============ YOUR CODE HERE ============\n",
    "    # 1. Set requires_grad attribute of tensor. Important for Attack\n",
    "\n",
    "    # 2. Forward pass the data through the model\n",
    "\n",
    "    # 3. Calculate the loss w.r.t to class predictions\n",
    "\n",
    "    # 4. Zero all existing gradients\n",
    "\n",
    "    # 5. Calculate gradients of model in backward pass\n",
    "\n",
    "    # 6. Collect sign of datagrad\n",
    "\n",
    "    # 7. Normalizing the gradient to the same space of image\n",
    "    sign_input_grad = sign_input_grad / 0.3081\n",
    "\n",
    "    # 8. Apply FGSM Attack\n",
    "\n",
    "    return perturbed_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd51390",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Compute predictions for ODIN on MNIST\n",
    "_, _, uncertainty_odin, errors_odin, _, _ = predict_test_set(lenet, test_loader, mode='odin', temp=5, epsilon=0.006)\n",
    "\n",
    "# Compute predictions for ODIN on KMNIST\n",
    "_, _, uncertainty_kmnist, errors_kmnist, _, _ = predict_test_set(lenet, kmnist_test_loader, mode='odin', temp=5, epsilon=0.0006)\n",
    "\n",
    "# Concatenating predictions with MNIST, considering KMNIST samples as out-of-distributions\n",
    "tot_uncertainty = np.concatenate((uncertainty_odin, uncertainty_kmnist))\n",
    "in_distribution = np.concatenate((np.zeros_like(uncertainty_odin), np.ones_like(uncertainty_kmnist)))\n",
    "\n",
    "# Obtaining precision and recall plot vector + AUPR\n",
    "precision_ood_odin, recall_ood_odin, _ = precision_recall_curve(in_distribution, -tot_uncertainty)\n",
    "aupr_ood_odin = average_precision_score(in_distribution, -tot_uncertainty)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de36e954",
   "metadata": {},
   "source": [
    "Letâ€™s look at the comparative results for failure prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e53a3d",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "plt.title('Precision-recall curve for OOD detection')\n",
    "plt.plot(recall_ood_mcp, precision_ood_mcp, label = f'MCP, AUPR = {aupr_ood_mcp:.2%}')\n",
    "plt.plot(recall_ood_ent, precision_ood_ent, label = f'MCDropout (MutInf), AUPR = {aupr_ood_ent:.2%}')\n",
    "plt.plot(recall_ood_odin, precision_ood_odin, label = f'ODIN, AUPR = {aupr_ood_odin:.2%}')\n",
    "plt.legend()\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b75bdd6",
   "metadata": {},
   "source": [
    "**[Question 3.1]**: Compare the precision-recall curves of each OOD method along with their AUPR values. Which method perform best and why?"
   ]
  }
 ],
 "metadata": {
  "date": 1770222286.369328,
  "filename": "robust_TP5.rst",
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python"
  },
  "title": "Practical session 5: Out-of-distribution detection"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}