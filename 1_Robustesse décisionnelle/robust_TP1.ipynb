{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "085d9291",
   "metadata": {},
   "source": [
    "\n",
    "<a id='chap-tprs1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f99ab7",
   "metadata": {},
   "source": [
    "# Practical session 1: Bayesian Linear Regression\n",
    "\n",
    "You can open this practical session in colab here :\n",
    "[https://colab.research.google.com/drive/1Uc-qCTOwhsnrvRyZ_ir75vgU10yRWZei?usp=sharing#scrollTo=lY10-bz2LKTN](https://colab.research.google.com/drive/1Uc-qCTOwhsnrvRyZ_ir75vgU10yRWZei?usp=sharing#scrollTo=lY10-bz2LKTN)\n",
    "\n",
    "or download the notebook directly [here](http://cedric.cnam.fr/~thomen/cours/RDFIA/TP1_Bayesian_Linear_Regression.ipynb).\n",
    "\n",
    "During this session, we will work with Bayesian Linear Regression models with varying basis functions (linear, polynomial and Gaussian). Datasets used are 1D toy regression samples ranging from linear datasets to more complex non-linear datasets such as increasing sinusoidal curves.\n",
    "\n",
    "Goal: Take hand on simple Bayesian models, understand how it works, gain finer insights on predictive distribution.\n",
    "\n",
    "First, lets import some usefull package and define plot functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a139a15",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline <- usefull for notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3780af7d",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Useful function: plot results\n",
    "def plot_results(X_train, y_train, X_test, y_test, y_pred, std_pred,\n",
    "             xmin=-2, xmax=2, ymin=-2, ymax=1, stdmin=0.30, stdmax=0.45):\n",
    "\"\"\"Given a dataset and predictions on test set, this function draw 2 subplots:\n",
    "- left plot compares train set, ground-truth (test set) and predictions\n",
    "- right plot represents the predictive variance over input range\n",
    "\n",
    "Args:\n",
    "  X_train: (array) train inputs, sized [N,]\n",
    "  y_train: (array) train labels, sized [N, ]\n",
    "  X_test: (array) test inputs, sized [N,]\n",
    "  y_test: (array) test labels, sized [N, ]\n",
    "  y_pred: (array) mean prediction, sized [N, ]\n",
    "  std_pred: (array) std prediction, sized [N, ]\n",
    "  xmin: (float) min value for x-axis on left and right plot\n",
    "  xmax: (float) max value for x-axis on left and right plot\n",
    "  ymin: (float) min value for y-axis on left plot\n",
    "  ymax: (float) max value for y-axis on left plot\n",
    "  stdmin: (float) min value for y-axis on right plot\n",
    "  stdmax: (float) max value for y-axis on right plot\n",
    "\n",
    "Returns:\n",
    "  None\n",
    "\"\"\"\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "plt.xlim(xmin = xmin, xmax = xmax)\n",
    "plt.ylim(ymin = ymin, ymax = ymax)\n",
    "plt.plot(X_test, y_test, color='green', linewidth=2,\n",
    "         label=\"Ground Truth\")\n",
    "plt.plot(X_train, y_train, 'o', color='blue', label='Training points')\n",
    "plt.plot(X_test, y_pred, color='red', label=\"BLR Poly\")\n",
    "plt.fill_between(X_test, y_pred-std_pred, y_pred+std_pred, color='indianred', label='1 std. int.')\n",
    "plt.fill_between(X_test, y_pred-std_pred*2, y_pred-std_pred, color='lightcoral')\n",
    "plt.fill_between(X_test, y_pred+std_pred*1, y_pred+std_pred*2, color='lightcoral', label='2 std. int.')\n",
    "plt.fill_between(X_test, y_pred-std_pred*3, y_pred-std_pred*2, color='mistyrose')\n",
    "plt.fill_between(X_test, y_pred+std_pred*2, y_pred+std_pred*3, color='mistyrose', label='3 std. int.')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"Predictive variance along x-axis\")\n",
    "plt.xlim(xmin = xmin, xmax = xmax)\n",
    "plt.ylim(ymin = stdmin, ymax = stdmax)\n",
    "plt.plot(X_test, std_pred**2, color='red', label=\"\\u03C3² {}\".format(\"Pred\"))\n",
    "\n",
    "# Get training domain\n",
    "training_domain = []\n",
    "current_min = sorted(X_train)[0]\n",
    "for i, elem in enumerate(sorted(X_train)):\n",
    "    if elem-sorted(X_train)[i-1]>1:\n",
    "        training_domain.append([current_min,sorted(X_train)[i-1]])\n",
    "        current_min = elem\n",
    "training_domain.append([current_min, sorted(X_train)[-1]])\n",
    "\n",
    "# Plot domain\n",
    "for j, (min_domain, max_domain) in enumerate(training_domain):\n",
    "    plt.axvspan(min_domain, max_domain, alpha=0.5, color='gray', label=\"Training area\" if j==0 else '')\n",
    "plt.axvline(X_train.mean(), linestyle='--', label=\"Training barycentre\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ebe467",
   "metadata": {},
   "source": [
    "## Part I: Linear Basis function model\n",
    "\n",
    "We start with a linear dataset where we will analyze the behavior of linear basis functions in the framework of Bayesian Linear Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b4a380",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "SIG = 0.2 #@param\n",
    "ALPHA = 2.0 #@param\n",
    "NB_POINTS =25 #@param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6408bb8c",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Generate linear toy dataset\n",
    "def f_linear(x, noise_amount, sigma):\n",
    "    y = -0.3 + 0.5*x\n",
    "    noise = np.random.normal(0, sigma, len(x))\n",
    "    return y + noise_amount*noise\n",
    "\n",
    "# Create training and test points\n",
    "dataset_linear = {}\n",
    "dataset_linear['X_train'] = np.random.uniform(0, 2, NB_POINTS)\n",
    "dataset_linear['y_train'] = f_linear(dataset_linear['X_train'], noise_amount=1, sigma=SIG)\n",
    "dataset_linear['X_test'] = np.linspace(-10,10, 10*NB_POINTS)\n",
    "dataset_linear['y_test'] = f_linear(dataset_linear['X_test'], noise_amount=0, sigma=SIG)\n",
    "dataset_linear['ALPHA'] = ALPHA\n",
    "dataset_linear['BETA'] = 1/(2.0*SIG**2)\n",
    "\n",
    "# Plot dataset\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.xlim(xmax = 3, xmin =-1)\n",
    "plt.ylim(ymax = 1.5, ymin = -1)\n",
    "plt.plot(dataset_linear['X_test'], dataset_linear['y_test'], color='green', linewidth=2, label=\"Ground Truth\")\n",
    "plt.plot(dataset_linear['X_train'], dataset_linear['y_train'], 'o', color='blue', label='Training points')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1a783b",
   "metadata": {},
   "source": [
    "We will use the linear basis function:\n",
    "\n",
    "$$\n",
    "\\phi:x \\rightarrow\n",
    "\\begin{pmatrix}\n",
    "1 \\\\ x\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Design matrix $ \\Phi $ defined on training set $ \\mathcal{D}=\\{(x_n, y_n)\\}_{n=1}^N $ is:\n",
    "\n",
    "$$\n",
    "\\Phi=\n",
    "\\begin{pmatrix}\n",
    "\\phi(x_1)^T \\\\\n",
    "... \\\\\n",
    "\\phi(x_N)^T\n",
    "\\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    "1 & x_1 \\\\\n",
    "... & ...\\\\\n",
    "1 & x_N\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "**[CODING TASK]** Define the linear basis function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f093c6a",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def phi_linear(x):\n",
    "    \"\"\" Linear Basis Functions\n",
    "\n",
    "    Args:\n",
    "    x: (float) scalar input\n",
    "\n",
    "    Returns:\n",
    "    (1D array) linear features of x\n",
    "    \"\"\"\n",
    "    # ============ YOUR CODE HERE ============\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112ea61a",
   "metadata": {},
   "source": [
    "**[Question 1.1]** Recall closed form of the posterior distribution in linear case.\n",
    "\n",
    "**[CODING TASK]** Compute mu and sigma to visualize posterior sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7287d1",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "for count,n in enumerate([0,1,2,10,len(dataset_linear['X_train'])]):\n",
    "    cur_data = dataset_linear['X_train'][:n]\n",
    "    cur_lbl = dataset_linear['y_train'][:n]\n",
    "    meshgrid = np.arange(-1, 1.01, 0.01)\n",
    "    w = np.zeros((2,1))\n",
    "    posterior = np.zeros((meshgrid.shape[0],meshgrid.shape[0]))\n",
    "\n",
    "    # ============ YOUR CODE HERE ============\n",
    "    # Compute sigma and mu\n",
    "    # Don't forget the special case of zero data\n",
    "    # points, i.e prior\n",
    "\n",
    "    # Compute values on meshgrid\n",
    "    for i in range(meshgrid.shape[0]):\n",
    "        for j in range(meshgrid.shape[0]):\n",
    "            w[0,0] = meshgrid[i]\n",
    "            w[1,0] = meshgrid[j]\n",
    "            posterior[i,j] = np.exp(-0.5* np.dot(np.dot((w-mu.reshape(2,1)).T, sigmainv) , (w-mu.reshape(2,1)) ) )\n",
    "    Z = 1.0 / ( np.sqrt(2*np.pi* np.linalg.det(sigma) ) )\n",
    "    posterior[:,:] /= Z\n",
    "\n",
    "    # Plot posterior with n points\n",
    "    plt.subplot(151+count)\n",
    "    plt.imshow(posterior, extent=[-1,1,-1,1])\n",
    "    plt.plot(0.5,0.3, '+', markeredgecolor='white', markeredgewidth=3, markersize=12)\n",
    "    plt.title('Posterior with N={} points'.format(n))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a572ee",
   "metadata": {},
   "source": [
    "**[Question 1.2]** Looking at the visualization of the posterior above, what can you say?\n",
    "\n",
    "**[Question 1.3]** Recall the closed form of the predictive distribution in linear case.\n",
    "\n",
    "**[CODING TASK]** Closed form solution according to the requirements on the left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b38ea7",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def closed_form(func, X_train, y_train, alpha, beta):\n",
    "\"\"\"Define analytical solution to Bayesian Linear Regression, with respect to the basis function chosen, the\n",
    "training set (X_train, y_train) and the noise precision parameter beta and prior precision parameter alpha chosen.\n",
    "It should return a function outputing both mean and std of the predictive distribution at a point x*.\n",
    "\n",
    "Args:\n",
    "  func: (function) the basis function used\n",
    "  X_train: (array) train inputs, size (N,)\n",
    "  y_train: (array) train labels, size (N,)\n",
    "  alpha: (float) prior precision parameter\n",
    "  beta: (float) noise precision parameter\n",
    "\n",
    "Returns:\n",
    "  (function) prediction function, returning itself both mean and std\n",
    "\"\"\"\n",
    "\n",
    "# ============ YOUR CODE HERE ============\n",
    "# Compute features and define mu_pred and sigma_pred\n",
    "\n",
    "def f_model(x):\n",
    "    # ============ YOUR CODE HERE ============\n",
    "    # Return closed form\n",
    "    return None\n",
    "\n",
    "return f_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201b9468",
   "metadata": {},
   "source": [
    "**[CODING TASK]** Predict on test dataset and visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bba007d",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Initialize predictive function\n",
    "f_pred = closed_form(phi_linear, dataset_linear['X_train'], dataset_linear['y_train'],\n",
    "                    dataset_linear['ALPHA'], dataset_linear['BETA'])\n",
    "\n",
    "# ============ YOUR CODE HERE ============\n",
    "# Predict test points and use visualization function\n",
    "# defined at the beginning of the notebook\n",
    "# You should use the following parameters for plot_results\n",
    "# xmin=-10, xmax=10, ymin=-6, ymax=6, stdmin=0.05, stdmax=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d9c1c6",
   "metadata": {},
   "source": [
    "**[Question 1.4]** Analyse these results. Describe the behavior of the predictive variance for points far from training distribution. Prove it analytically in the case where α=0 and β=1.\n",
    "\n",
    "**Bonus Question**: What happens when applying Bayesian Linear Regression on the following dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830c05c4",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Create training and test points\n",
    "dataset_hole = {}\n",
    "dataset_hole['X_train'] = np.concatenate(([np.random.uniform(-3, -1, 10), np.random.uniform(1, 3, 10)]), axis=0)\n",
    "dataset_hole['y_train'] = f_linear(dataset_hole['X_train'], noise_amount=1,sigma=SIG)\n",
    "dataset_hole['X_test'] = np.linspace(-12,12, 100)\n",
    "dataset_hole['y_test'] = f_linear(dataset_hole['X_test'], noise_amount=0,sigma=SIG)\n",
    "dataset_hole['ALPHA'] = ALPHA\n",
    "dataset_hole['BETA'] = 1/(2.0*SIG**2)\n",
    "\n",
    "# Plot dataset\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.xlim(xmin =-12, xmax = 12)\n",
    "plt.ylim(ymin = -7, ymax = 6)\n",
    "plt.plot(dataset_hole['X_test'], dataset_hole['y_test'], color='green', linewidth=2, label=\"Ground Truth\")\n",
    "plt.plot(dataset_hole['X_train'], dataset_hole['y_train'], 'o', color='blue', label='Training points')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a26914c",
   "metadata": {},
   "source": [
    "**[BONUS CODING TASK]** Define f_pred, predict on test points and plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d1f24b",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# TO DO: Define f_pred, predict on test points and plot results\n",
    "f_pred = closed_form(phi_linear, dataset_hole['X_train'], dataset_hole['y_train'],\n",
    "                    dataset_hole['ALPHA'], dataset_hole['BETA'])\n",
    "# ============ YOUR CODE HERE ============\n",
    "# Define new f_pred, then predict test points\n",
    "# and use visualization function as before\n",
    "# You should use the following parameters for plot_results\n",
    "# xmin=-12, xmax=12, ymin=-7, ymax=6, stdmin=0.0, stdmax=0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572ee414",
   "metadata": {},
   "source": [
    "## Part II: Non Linear models\n",
    "\n",
    "We now introduce a more complex toy dataset, which is an increasing sinusoidal curve. The goal of this part is to get insight on the importance of the chosen basis function on the predictive variance behavior.\n",
    "\n",
    "Hyperparameters for non-linear model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f62599",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "SIG = 0.2 #@param\n",
    "ALPHA = 0.05 #@param\n",
    "NB_POINTS = 50 #@param\n",
    "NB_POLYNOMIAL_FEATURES = 10 #@param\n",
    "MU_MIN = 0 #@param\n",
    "MU_MAX = 1 #@param\n",
    "NB_GAUSSIAN_FEATURES = 9 #@param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50cca96",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Generate sinusoidal toy dataset\n",
    "def f_sinus(x, noise_amount,sigma=0.2):\n",
    "    y = np.sin(2*np.pi*x) + x\n",
    "    noise = np.random.normal(0, sigma, len(x))\n",
    "    return y + noise_amount * noise\n",
    "\n",
    "# Create training and test points\n",
    "dataset_sinus = {}\n",
    "dataset_sinus['X_train'] = np.random.uniform(0, 1, NB_POINTS)\n",
    "dataset_sinus['y_train'] = f_sinus(dataset_sinus['X_train'], noise_amount=1,sigma=SIG)\n",
    "dataset_sinus['X_test'] = np.linspace(-1,2, 10*NB_POINTS)\n",
    "dataset_sinus['y_test'] = f_sinus(dataset_sinus['X_test'], noise_amount=0,sigma=SIG)\n",
    "dataset_sinus['ALPHA'] = ALPHA\n",
    "dataset_sinus['BETA'] = 1/(2.0*SIG**2)\n",
    "\n",
    "# Plot dataset\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.xlim(xmin =-1, xmax = 2)\n",
    "plt.ylim(ymin = -2, ymax = 3)\n",
    "plt.plot(dataset_sinus['X_test'], dataset_sinus['y_test'], color='green', linewidth=2,\n",
    "        label=\"Ground Truth\")\n",
    "plt.plot(dataset_sinus['X_train'], dataset_sinus['y_train'], 'o', color='blue', label='Training points')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e711cc84",
   "metadata": {},
   "source": [
    "### II.1 Polynomial basis functions\n",
    "\n",
    "We will first use polynomial basis functions:\n",
    "\n",
    "$$\n",
    "\\phi:x \\rightarrow\n",
    "\\begin{pmatrix}\n",
    "\\phi_0 \\\\\n",
    "... \\\\\n",
    "\\phi_{D-1}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "where $ \\phi_j = x^j $ for $ j \\geq 0 $ and $ D \\geq 0 $\n",
    "\n",
    "Design matrix \\$Phi\\$ defined on training set $ \\mathcal{D} $ is:\n",
    "\n",
    "$$\n",
    "\\Phi=\n",
    "\\begin{pmatrix}\n",
    "\\phi(x_1)^T \\\\\n",
    "... \\\\\n",
    "\\phi(x_n)^T\n",
    "\\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    "1 & x_1 & x_1^2 &... &x_1^{D-1} \\\\\n",
    "... & ... & ... & ...\\\\\n",
    "1 & x_N & x_N^2 &... &x_N^{D-1}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "**[CODING TASK]** Define basis function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce40193",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def phi_polynomial(x):\n",
    "\"\"\" Polynomial Basis Functions\n",
    "\n",
    "Args:\n",
    "  x: (float) scalar input\n",
    "\n",
    "Returns:\n",
    "  (1D-array) polynomial features of x\n",
    "\"\"\"\n",
    "\n",
    "# ============ YOUR CODE HERE ============"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2bd5b2",
   "metadata": {},
   "source": [
    "**[CODING TASK]** Implement closed form for polynomial features and visualise results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9cfe14",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# ============ YOUR CODE HERE ============\n",
    "# Define f_pred as the closed form for polynomial features\n",
    "# with a sinusoidal dataset\n",
    "# Then, predict test points and use visualization function\n",
    "# defined at the beginning of the notebook\n",
    "# You should use the following parameters for plot_results\n",
    "# xmin=-1, xmax=2, ymin=-3, ymax=5, stdmin=0, stdmax=10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3e90ef",
   "metadata": {},
   "source": [
    "**[Question 2.1]** What can you say about the predictive variance?\n",
    "\n",
    "Predictive variance increase as we move away from training data. However here with polynomial features, the minimum is not the training barycentre anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385e7eaa",
   "metadata": {},
   "source": [
    "### II.2 Gaussian basis functions\n",
    "\n",
    "Now, let’s consider gaussian basis functions:\n",
    "\n",
    "$$\n",
    "\\phi:x \\rightarrow\n",
    "\\begin{pmatrix}\n",
    "\\phi_0 \\\\\n",
    "... \\\\\n",
    "\\phi_{D-1}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "where $ \\phi_j = \\exp \\Big ( -\\frac{(x-\\mu_j)^2}{2s^2} \\Big ) $ for $ j \\geq 0 $\n",
    "\n",
    "Design matrix $ \\Phi $ defined on training set $ \\mathcal{D} $ is:\n",
    "\n",
    "$$\n",
    "\\Phi=\n",
    "\\begin{pmatrix}\n",
    "\\phi_0(x_1) & \\phi_1(x_1) &... &\\phi_{D-1}(x_1) \\\\\n",
    "... & ... & ... & ...\\\\\n",
    "\\phi_0(x_N) & \\phi_1(x_N) &... &\\phi_{D-1}(x_N)\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "e^{-\\frac{(x_1-\\mu_0)^2}{2s^2}} & e^{-\\frac{(x_1-\\mu_1)^2}{2s^2}} & ... & e^{-\\frac{(x_1 - \\mu_{D-1})^2}{2s^2}} \\\\\n",
    "... & ... & ... & ...\\\\\n",
    "e^{-\\frac{(x_N-\\mu_0)^2}{2s^2}} & e^{-\\frac{(x_N-\\mu_1)^2}{2s^2}} &... & e^{-\\frac{(x_N-\\mu_{D-1})^2}{2s^2}} \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "**[CODING TASK]** Define Gaussian basis function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921c068d",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def phi_gaussian(x) :\n",
    "    \"\"\" Gaussian Basis Functions defined linearly between\n",
    "    MU_MIN (=mu_0) and MU_MAX (=mu_{D-1})\n",
    "\n",
    "    Args:\n",
    "    x: (float) scalar input\n",
    "\n",
    "    Returns:\n",
    "    (1D-array) gaussian features of x\n",
    "    \"\"\"\n",
    "    s = (MU_MAX-MU_MIN)/ NB_GAUSSIAN_FEATURES\n",
    "\n",
    "    # ============ YOUR CODE HERE ============\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e42d317",
   "metadata": {},
   "source": [
    "**[CODING TASK]** Implement closed form for Gausian features and visualise results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a509846",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# ============ YOUR CODE HERE ============\n",
    "# Define f_pred as the closed form for Gaussian features\n",
    "# with a sinusoidal dataset\n",
    "# Then, predict test points and use visualization function\n",
    "# defined at the beginning of the notebook\n",
    "# You should use the following parameters for plot_results\n",
    "# xmin=-1, xmax"
   ]
  }
 ],
 "metadata": {
  "date": 1770222286.3043296,
  "filename": "robust_TP1.rst",
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python"
  },
  "title": "Practical session 1: Bayesian Linear Regression"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}