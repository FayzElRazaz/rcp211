{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "106432b5",
   "metadata": {},
   "source": [
    "\n",
    "<a id='chap-tprs4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ceafd1",
   "metadata": {},
   "source": [
    "# Practical session 4: Uncertainty in classification\n",
    "\n",
    "You can open this practical session in colab here :\n",
    "[https://colab.research.google.com/drive/1WrzZsbsBo0xe6Zs-XAc3Rj083Ur1fzpP?usp=sharing](https://colab.research.google.com/drive/1WrzZsbsBo0xe6Zs-XAc3Rj083Ur1fzpP?usp=sharing)\n",
    "\n",
    "or download the notebook directly [here](http://cedric.cnam.fr/~rambourc/TP3_RCP211_Bayesian_NN.ipynb).\n",
    "\n",
    "This lab session will focus on applications based on uncertainty estimation. We will first use MC Dropout variational inference to qualitatively evaluate the most uncertain images according to the mode. Then, we’ll move to failure prediction.\n",
    "\n",
    "**Goal**: Take hand on applying uncertainty estimation for failure prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afc6db5",
   "metadata": {},
   "source": [
    "## All Imports and Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d644cbfc",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from IPython import display\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib.pyplot import imread\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "use_cuda = True\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "kwargs = {}\n",
    "#kwargs = {'num_workers': 10, 'pin_memory': True} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b07034",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "#Useful plot function\n",
    "\n",
    "def plot_predicted_images(selected_idx, images, pred, labels, uncertainties, hists, mc_samples):\n",
    "    \"\"\" Plot predicted images along with mean-pred probabilities histogram, maxprob frequencies and some class histograms\n",
    "    across sampliong\n",
    "\n",
    "    Args:\n",
    "    selected_ix: (array) chosen index in the uncertainties tensor\n",
    "    images: (tensor) images from the test set\n",
    "    pred: (tensor) class predictions by the model\n",
    "    labels: (tensor) true labels of the given dataset\n",
    "    uncertainties: (tensor) uncertainty estimates of the given dataset\n",
    "    errors: (tensor) 0/1 vector whether the model wrongly predicted a sample\n",
    "    hists : (array) number of occurences by class in each sample fo the given dataset, only with MCDropout\n",
    "    mc_samples: (tensor) prediction matrix for s=100 samples, only with MCDropout\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15,10))\n",
    "\n",
    "    for i, idx in enumerate(selected_idx):\n",
    "        # Plot original image\n",
    "        plt.subplot(5,6,1+6*i)\n",
    "        plt.axis('off')\n",
    "        plt.title(f'var-ratio={uncertainties[idx]:.3f}, \\n gt={labels[idx]}, pred={pred[idx]}')\n",
    "        plt.imshow(images[idx], cmap='gray')\n",
    "\n",
    "        # Plot mean probabilities\n",
    "        plt.subplot(5,6,1+6*i+1)\n",
    "        plt.title('Mean probs')\n",
    "        plt.bar(range(10), mc_samples[idx].mean(0))\n",
    "        plt.xticks(range(10))\n",
    "\n",
    "        # Plot frequencies\n",
    "        plt.subplot(5,6,1+6*i+2)\n",
    "        plt.title('Maxprob frequencies')\n",
    "        plt.bar(range(10), hists[idx])\n",
    "        plt.xticks(range(10))\n",
    "\n",
    "        # Plot probs frequency for specific class\n",
    "        list_plotprobs = [hists[idx].argsort()[-1], hists[idx].argsort()[-2], hists[idx].argsort()[-4]]\n",
    "        ymax = max([max(np.histogram(mc_samples[idx][:,c])[0]) for c in list_plotprobs])\n",
    "        for j, c in enumerate(list_plotprobs):\n",
    "            plt.subplot(5,6,1+6*i+(3+j))\n",
    "            plt.title(f'Samples probs of class {c}')\n",
    "            plt.hist(mc_samples[idx][:,c], bins=np.arange(0,1.1,0.1))\n",
    "            plt.ylim(0,np.ceil(ymax/10)*10)\n",
    "            plt.xticks(np.arange(0,1,0.1), rotation=60)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b888e7f",
   "metadata": {},
   "source": [
    "## Part I: Monte-Carlo Dropout on MNIST\n",
    "\n",
    "By appling MC Dropout variational inference method, we’re interested to obtain an uncertainty\n",
    "measure which can be use to spot the most uncertain images in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245d4aab",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,),(0.3081,))])\n",
    "train_dataset = datasets.MNIST('data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, **kwargs)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, **kwargs)\n",
    "\n",
    "# Visualize some images\n",
    "images, labels = next(iter(train_loader))\n",
    "fig, axes = plt.subplots(nrows=4, ncols=4)\n",
    "for i, (image, label) in enumerate(zip(images, labels)):\n",
    "    if i >= 16:\n",
    "        break\n",
    "    axes[i // 4][i % 4].imshow(images[i][0], cmap='gray')\n",
    "    axes[i // 4][i % 4].set_title(f\"{label}\")\n",
    "    axes[i // 4][i % 4].set_xticks([])\n",
    "    axes[i // 4][i % 4].set_yticks([])\n",
    "fig.set_size_inches(4, 4)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dff1832",
   "metadata": {},
   "source": [
    "### I.1 LeNet-5 network with dropout layers\n",
    "\n",
    "We will use a model in the style of LeNet-5 to implement Monte-Carlo dropout variational inference.\n",
    "\n",
    "<img src=\"Bloc_Robust/images/lenet.png\" alt=\"Alternative text\" style=\"width:600;\">\n",
    "\n",
    "Compared to the previous figure, the model we will implement will be defined as :\n",
    "\n",
    "- a convolutional layer with 6 channels, kernel size 5, padding 2 and ReLU activation  \n",
    "- a max pooling layer with kernel size 2  \n",
    "- a convolutional layer with 16 channels, kernel size 5  and ReLU activation  \n",
    "- a max pooling layer with kernel size 2  \n",
    "\n",
    "\n",
    "Then flatten and:\n",
    "\n",
    "- a dropout layer with p=0.25  \n",
    "- a fully-connected layer of size 120 and ReLU activation  \n",
    "- a dropout layer with p=0.5  \n",
    "- a final fully-connected layer of size 10 and ReLU activation  \n",
    "\n",
    "\n",
    "**[CODING TASK]** Implement a LeNet5-style neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db5832c",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "class LeNet5(nn.Module):\n",
    "    def __init__(self, n_classes=10):\n",
    "        super().__init__()\n",
    "        # ============ YOUR CODE HERE ============\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ============ YOUR CODE HERE ============\n",
    "        # Be careful, the dropout layer should be also\n",
    "        # activated during test time.\n",
    "        #(Hint: we may want to look out at F.dropout())\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bd3076",
   "metadata": {},
   "source": [
    "Now let’s train our model for 20 epochs using cross-entropy loss as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b36f42",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "lenet = LeNet5(n_classes=len(train_dataset.classes)).to(device)\n",
    "lenet.train()\n",
    "optimizer = torch.optim.SGD(lenet.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for i in range(20):\n",
    "    total_loss, correct = 0.0, 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = lenet(images)\n",
    "        loss = criterion(output, labels)\n",
    "        total_loss += loss\n",
    "        correct += (output.argmax(-1) == labels).sum()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"[Epoch {i + 1:2d}] loss: {total_loss/ len(train_dataset):.2E} accuracy_train: {correct / len(train_dataset):.2%}\")\n",
    "torch.save(lenet.state_dict(), 'lenet_final.cpkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d125d1b9",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# If you already train your model, you can load it instead using :\n",
    "lenet.load_state_dict(torch.load('lenet_final.cpkt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709a954e",
   "metadata": {},
   "source": [
    "### I.2 Investigating most uncertain samples\n",
    "\n",
    "For classification, there exists a few measures to compute uncertainty estimates:\n",
    "- **var-ratios**: collect the predicted label for each stochastic forward pass. Find the most sampled label and compute:\n",
    "\n",
    "$$\n",
    "\\text{variation-ratio}[x] = 1 - \\frac{f_x}{T}\n",
    "$$\n",
    "\n",
    "where $ f_x $ is the frequency of the chosen label and \\$T\\$ the number of pass.\n",
    "\n",
    "- **entropy**: captures the average amount of information contained in the predictive distribution.  \n",
    "\n",
    "\n",
    "$$\n",
    "\\mathcal{H}[y \\vert x, \\mathcal{D}] = - \\sum_c \\Big ( \\frac{1}{T} \\sum_t p(y=c \\vert x, w_t) \\Big ) \\log \\Big ( \\frac{1}{T} \\sum_t p(y=c \\vert x, w_t) \\Big )\n",
    "$$\n",
    "\n",
    "- **mutual information** : points that maximise the mutual informations are points on which the model is uncertain on average  \n",
    "\n",
    "\n",
    "$$\n",
    "\\mathcal{MI}[y,w \\vert x, \\mathcal{D}] = \\mathcal{H}[y \\vert x, \\mathcal{D}]+ \\frac{1}{T} \\sum_{c,t} p(y=c \\vert x, w_t) \\log p(y=c \\vert x, w_t)\n",
    "$$\n",
    "\n",
    "**[CODING TASK]** Implement variational-ratio, entropy and mutual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daa7106",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def predict_test_set(model, test_loader, mode='mcp', s=100, temp=5, epsilon=0.0006, verbose=True):\n",
    "    \"\"\"Predict on a test set given a model\n",
    "    # and a chosen method to compute uncertainty estimate\n",
    "    # (mcp, MC-dropout with var-ratios/entropy/mutual information\n",
    "    # ConfidNet and ODIN)\n",
    "\n",
    "    Args:\n",
    "    model: (nn.Module) a trained model\n",
    "    test_loader: (torch.DataLoader) a Pytorch dataloader based on a dataset\n",
    "    mode: (str) chosen uncertainty estimate method (mcp, var-ratios, entropy, mi, odin)\n",
    "    s: (int) number of samples in MCDropout\n",
    "    temp: (int, optional) value of T for temperature scaling in ODIN\n",
    "    epsilon: (float, optional) value of epsilon for inverse adversarial perturbation in ODIN\n",
    "    verbose: (bool, optional) printing progress bar when predicting\n",
    "\n",
    "    Returns:\n",
    "    pred: (tensor) class predictions by the model\n",
    "    labels: (tensor) true labels of the given dataset\n",
    "    uncertainties: (tensor) uncertainty estimates of the given dataset\n",
    "    errors: (tensor) 0/1 vector whether the model wrongly predicted a sample\n",
    "    hists : (array) number of occurences by class in each sample fo the given dataset, only with MCDropout\n",
    "    mc_samples: (tensor) prediction matrix for s=100 samples, only with MCDropout\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    preds, uncertainties, labels, errors  = [], [], [], []\n",
    "    mc_samples, hists = [], []\n",
    "\n",
    "    loop = tqdm(test_loader, disable=not verbose)\n",
    "    for images, targets in loop:\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "\n",
    "        if mode in ['mcp','odin']:\n",
    "            model.training = False\n",
    "            if mode=='odin':\n",
    "                # Coding task in Section 3: implement ODIN\n",
    "                images = odin_preprocessing(model,images,epsilon).to(device)\n",
    "            with torch.no_grad():\n",
    "                output = model(images)\n",
    "            if isinstance(output,tuple):\n",
    "                output = output[0]\n",
    "            if mode =='odin':\n",
    "                output = output / temp\n",
    "            confidence, pred = F.softmax(output, dim=1).max(dim=1, keepdim=True)\n",
    "            confidence = confidence.detach().to('cpu').numpy()\n",
    "\n",
    "        elif mode in ['var-ratios', 'entropy', 'mut_inf']:\n",
    "            model.training = True\n",
    "            outputs = torch.zeros(images.shape[0], s, 10)\n",
    "            for i in range(s):\n",
    "                with torch.no_grad():\n",
    "                    outputs[:,i] = model(images)\n",
    "            mc_probs = F.softmax(outputs, dim=2)\n",
    "            predicted_class = mc_probs.max(dim=2)[1]\n",
    "            pred = mc_probs.mean(1).max(dim=1, keepdim=True)[1]\n",
    "            mc_samples.extend(mc_probs)\n",
    "            hist = np.array([np.histogram(predicted_class[i,:], range=(0,10))[0]\n",
    "                            for i in range(predicted_class.shape[0])])\n",
    "            hists.extend(hist)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # ============ YOUR CODE HERE ============\n",
    "            # hist : histogram of the predicted class for each sampling pass\n",
    "            #        ie. what you need to compute the var-ratios\n",
    "            # mc_probs : contains the output probability (softmax) for each MC pass.\n",
    "            #            This is what you need to average over all the sampling pass\n",
    "            #            to compute the entropy and the mutual information\n",
    "\n",
    "            if mode=='var-ratios':\n",
    "                # You may want to use the hist variable here\n",
    "                confidence = None # <-- your implementation here>\n",
    "            elif mode=='entropy':\n",
    "                confidence = None # <-- your implementation here>\n",
    "            elif mode=='mut_inf':\n",
    "                confidence = None # <-- your implementation here>\n",
    "            # =======================================\n",
    "\n",
    "        elif mode=='confidnet':\n",
    "        with torch.no_grad():\n",
    "            output, confidence = model(images)\n",
    "        _, pred = F.softmax(output, dim=1).max(dim=1, keepdim=True)\n",
    "        confidence = confidence.detach().to('cpu').numpy()\n",
    "\n",
    "        preds.extend(pred)\n",
    "        labels.extend(targets)\n",
    "        uncertainties.extend(confidence)\n",
    "        errors.extend((pred.to(device)!=targets.view_as(pred)).detach().to(\"cpu\").numpy())\n",
    "\n",
    "    preds = np.reshape(preds, newshape=(len(preds), -1)).flatten()\n",
    "    labels = np.reshape(labels, newshape=(len(labels), -1)).flatten()\n",
    "    uncertainties = np.reshape(uncertainties, newshape=(len(uncertainties), -1)).flatten()\n",
    "    errors = np.reshape(errors, newshape=(len(errors), -1)).flatten()\n",
    "\n",
    "    if mode in ['var-ratios', 'entropy', 'mi']:\n",
    "        hists = np.reshape(hists, newshape=(len(hists), -1))\n",
    "\n",
    "    print(f'Test set accuracy = {(preds == labels).sum()/len(preds):.2%}')\n",
    "\n",
    "    return preds, labels, uncertainties, errors, hists, mc_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39a2039",
   "metadata": {},
   "source": [
    "Now let’s compute uncertainty estimates on the test set to visualize the most uncertainty samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eff24af",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Predicting along with var-ratios uncertainty estimates\n",
    "pred_var, labels, uncertainty_var, errors_var, \\\n",
    "    hists, mc_samples = predict_test_set(lenet, test_loader, mode='var-ratios')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1dac01",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Plotting random images with their var-ratios value\n",
    "random_samples = np.random.choice(uncertainty_var.shape[0], 2, replace=False)\n",
    "plot_predicted_images(random_samples, test_loader.dataset.data, pred_var, labels, uncertainty_var, hists, mc_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce48231",
   "metadata": {},
   "source": [
    "We compare this random sample to the most uncertain images according to the var-ratio.\n",
    "\n",
    "**[CODING TASK]** Visualize the top-3 most uncertain images along with their var-ratios value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17b8330",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# ============ YOUR CODE HERE ============\n",
    "# Re-use the function 'plot_predicted_images' to visualize\n",
    "# results.\n",
    "top_uncertain_samples = None # <-- based on the values in uncertainty_var. You can use the .argsort() method>\n",
    "plot_predicted_images(top_uncertain_samples, test_loader.dataset.data,\n",
    "                    pred_var, labels, uncertainty_var, hists, mc_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049fa4cd",
   "metadata": {},
   "source": [
    "\n",
    "<dl style='margin: 20px 0;'>\n",
    "<dt>**[Question 1.1]**: What can you say about the images themselfs. How</dt>\n",
    "<dd>\n",
    "do the histograms along them helps to explain failure cases? Finally, how\n",
    "do probabilities distribution of random images compare to the previous top uncertain images?\n",
    "\n",
    "</dd>\n",
    "\n",
    "</dl>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fccee1",
   "metadata": {},
   "source": [
    "## Part II: Failure prediction\n",
    "\n",
    "The objective is to provide confidence measures for model’s predictions that are\n",
    "reliable and whoseranking among samples enables to distinguish correct from incorrect predictions.\n",
    "Equipped with sucha confidence measure, a system could decide to stick to the prediction or, on the\n",
    "contrary, to handover to a human or a back-up system with,e.g.other sensors, or simply to trigger an alarm.\n",
    "\n",
    "<img src=\"Bloc_Robust/images/confidence.png\" alt=\"Alternative text\" style=\"width:600;\">\n",
    "\n",
    "We will introduce ConfidNet, a specific method design to address failure prediction and we will compare it to MCDropout with entropy and Maximum Class Probability (MCP)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dd0b01",
   "metadata": {},
   "source": [
    "### II.1 ConfidNet\n",
    "\n",
    "By taking the largest softmax probability as confidence estimate,\n",
    "MCP leads to high confidence values both for correct and erroneous predictions\n",
    "alike. On the other hand, when the model misclassifies an example, the probability\n",
    "associated to the true class $ y` $ is lower than the maximum one and likely to be low.\n",
    "\n",
    "Based on this observation, we can consider instead the **True Class Probability**\n",
    "as a suitable uncertainty criterion.\n",
    "For any admissible input $ \\pmb{x}\\in \\mathcal{X} $, we assume the *true* class\n",
    "$ y(\\pmb{x}) $ is known, which we denote $ y $ for simplicity.\n",
    "The TCP of a model \\$F\\$ is defined as\n",
    "\n",
    "$$\n",
    "\\text{TCP}_F(\\pmb{x},y) = P(Y=y \\vert \\pmb{x}, \\hat{\\pmb{w}})\n",
    "$$\n",
    "\n",
    "**Theoretical guarantees.** Given a properly labelled example  $ (\\pmb{x},y) $, then:\n",
    "\n",
    "- $ \\text{TCP}_F(\\pmb{x},y)> 1/2 $ $ \\Rightarrow f(\\pmb{x}) = y $,\n",
    "  *i.e.* the example is correctly classified by the model;%the example has been correctly classified,  \n",
    "- $ \\text{TCP}_F(\\pmb{x},y) < 1/K $ $ \\Rightarrow f(\\pmb{x}) \\neq y $,\n",
    "  *i.e.* the example is wrongly classified by the model.  \n",
    "\n",
    "\n",
    "However, the true classes \\$y\\$ are obviously not available\n",
    "when estimating confidence on test inputs. Alternatively, we\n",
    "can **learn TCP criterion from data** with an auxiliary model called **ConfidNet**.\n",
    "\n",
    "ConfidNet is designed as a small multilayer perceptron composed of\n",
    "a succession of dense layers with a final sigmoid activation that\n",
    "outputs $ C(\\pmb{x};\\pmb{\\theta})\\in[0,1] $. We use a mean-square-error (MSE) loss to train this model:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{conf}}(\\pmb{\\theta};\\mathcal{D}) = \\frac{1}{N} \\sum_{n=1}^N \\big(C(\\pmb{x}_n;\\pmb{\\theta}) - \\text{TCP}_F(\\pmb{x}_n,y_n)\\big)^2.\n",
    "$$\n",
    "\n",
    "<img src=\"Bloc_Robust/images/confidnet.jpeg\" alt=\"Alternative text\" style=\"width:600;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5041f9e7",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "class LeNet5ConfidNet(nn.Module):\n",
    "    ''' A LeNet5-syle model equipped with ConfidNet auxiliary branch '''\n",
    "    def __init__(self, n_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5, padding=2)\n",
    "        self.maxpool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5, padding=0)\n",
    "        self.maxpool2 = nn.MaxPool2d(2)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(120, n_classes)\n",
    "\n",
    "        # ConfidNet Layers\n",
    "        self.uncertainty1 = nn.Linear(120, 400)\n",
    "        self.uncertainty2 = nn.Linear(400, 400)\n",
    "        self.uncertainty3 = nn.Linear(400, 400)\n",
    "        self.uncertainty4 = nn.Linear(400, 400)\n",
    "        self.uncertainty5 = nn.Linear(400, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = self.maxpool1(out)\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = self.maxpool2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.dropout1(out)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = self.dropout2(out)\n",
    "\n",
    "        # Uncertainty prediction\n",
    "        uncertainty = F.relu(self.uncertainty1(out))\n",
    "        uncertainty = F.relu(self.uncertainty2(uncertainty))\n",
    "        uncertainty = F.relu(self.uncertainty3(uncertainty))\n",
    "        uncertainty = F.relu(self.uncertainty4(uncertainty))\n",
    "        uncertainty = self.uncertainty5(uncertainty)\n",
    "\n",
    "        pred = self.fc2(out)\n",
    "        return pred, uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad218ab",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "class SelfConfidMSELoss(nn.modules.loss._Loss):\n",
    "    ''' MSE Loss for confidence learning '''\n",
    "    def __init__(self, num_classes,device):\n",
    "        self.nb_classes = num_classes\n",
    "        self.device = device\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        probs = F.softmax(input[0], dim=1)\n",
    "        confidence = torch.sigmoid(input[1]).squeeze()\n",
    "        labels_hot = torch.eye(10)[target.flatten()].to(device)\n",
    "        loss = (confidence - (probs * labels_hot).sum(dim=1)) ** 2\n",
    "        return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d3fa79",
   "metadata": {},
   "source": [
    "We train only the ConfidNet layers for 30 epochs. During confidence learning, original classification layers are fixed to keep predictions unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7ae0ee",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "lenet_confidnet = LeNet5ConfidNet(n_classes=10).to(device)\n",
    "lenet_confidnet.load_state_dict(torch.load('lenet_final.cpkt'), strict=False)\n",
    "lenet_confidnet.train()\n",
    "optimizer = torch.optim.Adam(lenet_confidnet.parameters(), lr=1e-4)\n",
    "criterion = SelfConfidMSELoss(10,device)\n",
    "\n",
    "# Freezing every layer except uncertainty for confidence training\n",
    "for param in lenet_confidnet.named_parameters():\n",
    "    if \"uncertainty\" in param[0]:\n",
    "        continue\n",
    "    param[1].requires_grad = False\n",
    "\n",
    "for i in range(30):\n",
    "    lenet_confidnet.train()\n",
    "    # Fine-tuning without stochasticity\n",
    "    if i>20:\n",
    "        # Keeping original batch norm values\n",
    "        for layer in lenet_confidnet.named_modules():\n",
    "            if isinstance(layer[1], torch.nn.BatchNorm2d):\n",
    "                layer[1].momentum = 0\n",
    "                layer[1].eval()\n",
    "        # Disabling dropout\n",
    "        for layer in lenet_confidnet.named_modules():\n",
    "            if \"dropout\" in layer[0]:\n",
    "                layer[1].eval()\n",
    "\n",
    "    total_loss, correct, best_aupr = 0.0, 0.0, 0.0\n",
    "    errors, uncertainty = [], []\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = lenet_confidnet(images)\n",
    "        probs = F.softmax(output[0], dim=1)\n",
    "        pred = probs.max(dim=1)[1]\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss\n",
    "        correct += (pred == labels).sum()\n",
    "        errors.extend((pred != labels.view_as(pred)).detach().to(\"cpu\").numpy())\n",
    "        uncertainty.extend(output[1].squeeze().detach().to(\"cpu\").numpy())\n",
    "\n",
    "    errors = np.reshape(errors, newshape=(len(errors), -1)).flatten()\n",
    "    uncertainty = np.reshape(uncertainty, newshape=(len(uncertainty), -1)).flatten()\n",
    "    aupr = average_precision_score(errors, -uncertainty)\n",
    "    print(f\"[Epoch {i + 1}] loss: {total_loss/ len(train_dataset):.2E}\"+\n",
    "        f\"\\t accuracy_train: {correct / len(train_dataset):.2%}\"+\n",
    "        f\"\\t aupr_train: {aupr:.2%}\")\n",
    "    if aupr>best_aupr:\n",
    "    best_aupr = aupr\n",
    "    torch.save(lenet_confidnet.state_dict(), 'lenet_confidnet_best.cpkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e05b391",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# If you already train your model, you can load it instead using :\n",
    "# lenet_confidnet.load_state_dict(torch.load('lenet_confidnet_best.cpkt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da5e112",
   "metadata": {},
   "source": [
    "### II.2 Evaluate failure prediction performances\n",
    "\n",
    "We compare the capacity of ConfidNet to detect failures with previous baselines (MCP and MCDropout with entropy).\n",
    "\n",
    "To measure performances, we use the *Area under the Precision-Recall*\n",
    "curve (AUPR). The precision-recall (PR) curve is the graph of the precision\n",
    "$ = \\mathrm{TP}/(\\mathrm{TP} + \\mathrm{FP}) $ as a function of the recall $ = \\mathrm{TP}/(\\mathrm{TP} + \\mathrm{FN}) $\n",
    "where $ \\mathrm{TP} $, $ \\mathrm{TN} $, $ \\mathrm{FP} $ and $ \\mathrm{FN} $\n",
    "are the numbers of true positives, true negatives, false positives and false negatives\n",
    "respectively. In our experiments, classification errors are used as the positive detection class.\n",
    "\n",
    "> **[CODING TASK]** Compute precision and recall vectors along with AUPR score for ConfidNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0c0ec3",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# ============ YOUR CODE HERE ============\n",
    "# We use predict_test_set function to obtain confidence estimates\n",
    "# with previous model, choosing 'confidnet' mode.\n",
    "# Then calculate the precision, recall and aupr\n",
    "# with sklearn functions.\n",
    "# /!\\ In failure prediction, errors are consider\n",
    "# as the positive class\n",
    "\n",
    "_, _, uncertainty_confidnet, errors_confidnet, _, _ = predict_test_set(lenet_confidnet, test_loader, mode='confidnet')\n",
    "\n",
    "aupr_confidnet = # <-- use average_precision_score()\n",
    "precision_confidnet, recall_confidnet, _ = # <-- use precision_recall_curve()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b79f97",
   "metadata": {},
   "source": [
    "**[CODING TASK]** Same with MCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfae93d6",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# ============ YOUR CODE HERE ============\n",
    "# Mode = 'mcp'\n",
    "\n",
    "_, _, uncertainty_mcp, errors_mcp, _, _ = predict_test_set(lenet, test_loader, mode='mcp')\n",
    "aupr_mcp = # <-- use average_precision_score()\n",
    "precision_mcp, recall_mcp, _ = # <-- use precision_recall_curve()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106a6704",
   "metadata": {},
   "source": [
    "**[CODING TASK]** Same with MCDropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d914da",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# ============ YOUR CODE HERE ============\n",
    "# Mode = 'entropy'\n",
    "\n",
    "_, _, uncertainty_ent, errors_ent, _, _ = predict_test_set(lenet, test_loader, mode='entropy')\n",
    "aupr_ent = # <-- use average_precision_score()\n",
    "precision_ent, recall_ent, _ = # <-- use precision_recall_curve()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f407f1b",
   "metadata": {},
   "source": [
    "Let’s look at the comparative results for failure prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661687b0",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "plt.plot(recall_mcp, precision_mcp, label = f'MCP, AUPR = {aupr_mcp:.2%}')\n",
    "plt.plot(recall_ent, precision_ent, label = f'MCDropout (Entropy), AUPR = {aupr_ent:.2%}')\n",
    "plt.plot(recall_confidnet, precision_confidnet, label = f'ConfidNet, AUPR = {aupr_confidnet:.2%}')\n",
    "plt.title('Precision-recall curves for failure prediction')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05937d37",
   "metadata": {},
   "source": [
    "**[Question 2.1]**: Compare the precision-recall curves of each method along with their\n",
    "AUPR values. Why did we use AUPR metric instead of standard AUROC?"
   ]
  }
 ],
 "metadata": {
  "date": 1770222286.3591132,
  "filename": "robust_TP4.rst",
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python"
  },
  "title": "Practical session 4: Uncertainty in classification"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}